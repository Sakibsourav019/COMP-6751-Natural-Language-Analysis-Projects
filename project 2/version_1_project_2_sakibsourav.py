# -*- coding: utf-8 -*-
"""Version 1_Project_2_SakibSourav.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ps0xa8tx5Zhc_6lke1kPFBuC3Jz_sk8r
"""

tagset = {
  # Coordinating conjunction
  "CC": ["but", "and", "that", "against", "both", "which", "or"],

  # Cardinal number
  "CD": [],

  # Determiner
  "DT": ["a", "an", "the"],

  # Existential there
  "EX": [],

  # Foreign word
  "FW": [],

  # Preposition or subordinating conjunction
  "IN": ["in", "on", "at", "from", "in", "on", "at", "from", "to", "with", "against", "for", "of"],

  # Adjective
  "JJ": ["healthy", "sick", "happy", "sad", "angry", "productive", "lazy", "his", "her", "their", "your", "last", "promising", "that", "those", "crunchy", "both", "semantic", "deep", "dutch", "several", "some", "further", "logical", "two"],

  # Adjective, comparative
  "JJR": ["healthier", "sicker", "happier", "sadder", "angrier", "lazier", "crunchier", "deeper", "90"],

  # Adjective, superlative
  "JJS": ["healthiest", "sickest", "happiest", "saddest", "angriest", "laziest", "crunchiest", "deepest"],

  # List item marker
  "LS": [],

  # Modal
  "MD": [],

  # Noun, singular or mass
  "NN": ["apple", "PhD", "mango", "orange", "banana", "heart", "rate", "beats/minute", "breaths/minute", "/microliters", "mmhg", "leukocyte", "count", "immature", "forms", "bands", "body", "temperature", "degrees", "celsius","respiratory", "rate", "partial", "pressure", "co2", "grape", "table", "chair", "bat", "ball", "fridge", "office", "morning", "flight", "yesterday", "today", "tomorrow", "refrigerator", "week", "last", "will", "desk", "colleague", "replacement", "day", "everyday", "thesis", "representation", "representations", "parser", "sentences", "issues", "improvements", "system", "generator", "style", "form", "inference", "two", "ways"],

  # Noun, plural
  "NNS": [],

  # Proper noun, singular
  "NNP": ["Sakib", "Huda", "John", "Sue", "James", "Robert", "Dutch", "Mary", "Patricia", "Jennifer", "O-Malley", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday", "Janurary", "Febuary", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December", "Delilah"],

  # Proper noun, plural
  "NNPS": [],

  # Predeterminer
  "PDT": [],

  # Possessive ending
  "POS": [],

  # Personal pronoun
  "PRP": ["I", "it", "he", "she", "they", "that" ,"them", "both", "this", "some", "several"],

  # Possessive pronoun
  "PRPS": [],

  # Adverb
  "RB": ["not", "finally", "last", "some", "further", "mainly", "two", "deep"],

  # Adverb, comparative
  "RBR": [],

  # Adverb, superlative
  "RBS": [],

  # Particle
  "RP": [],

  # Symbol
  "SYM": [],

  # to
  "TO": [],

  # Interjection
  "UH": [],

  # Verb, base form
  "VB": ["eat", "prefer", "be", "last", "over", "under", "take", "greater", "than", "less", "than", "promise", "will", "put", "intend", "share", "anticipate", "treat", "delight", "say", "focus", "parse"],

  # Verb, past tense
  "VBD": ["ate", "preferred", "was", "were", "lasted", "took", "promised", "would", "put", "intended", "shared", "anticipated", "treated", "delighted", "said", "focussed", "focused", "parsed", "developed", "optimized"],

  # Verb, gerund or present participle
  "VBG": ["eating", "preferring", "being", "lasting", "taking", "promising", "putting", "intending", "sharing", "anticipating", "treating", "delighting", "saying", "focussing", "focusing", "parsing", "discussing", "proposing"],

  # Verb, past participle
  "VBN": ["eaten", "preferred", "been", "lasted", "taken"],

  # Verb, non-3rd person singular present
  "VBP": ["am", "are", "will"],

  # Verb, 3rd person singular present
  "VBZ": ["is", "eats", "prefers", "lasts", "takes", "promises", "intends", "shares", "anticipates", "treats", "delights", "says", "focuses", "parses", "describes", "computes"],

  # Wh-determiner
  "WDT": [],

  # Wh-pronoun
  "WP": ["who", "which"],

  # 	Possessive wh-pronoun
  "WPS": [],

  # Wh-adverb
  "WRB": [],

}

# Including numbers 0 to 100 as common nouns
[tagset["NN"].append(str(x)) for x in range(101)]

# Including numbers 1000 to 20230 as common nouns
[tagset["NN"].append("20"+str(x)) for x in range(24)]

"""# Parse Tree Generation"""

import nltk
from nltk import CFG
from nltk.tokenize import RegexpTokenizer
import string
# from tagset import tagset

from nltk.tree import Tree
from nltk.tree.prettyprinter import TreePrettyPrinter

def Tokenizer(sentence):
  """
  Tokenizes a given NLTK Corpus file into individual words.

  Arguments:
    file: NLTK Corpus file

  Returns:
    List containing tokens from the NLTK Corpus file.
  """

  # Defining RegEx rules for NLTK Tokenizer
  NLTKRegexpTokenizer = RegexpTokenizer(r'''(?x)
  \S+|\$[\d\.]+ # Split by words
  ''')
  tokens = NLTKRegexpTokenizer.tokenize(sentence)

  # Improvement: Detect and separate tokens with punctuations at the end
  i=0
  while i in range(len(tokens)):
    if tokens[i][-1] in string.punctuation:
      tokens.insert(i+1, tokens[i][-1])
      tokens[i] = tokens[i][:-1]
      i+=1

    i+=1

  return tokens


def earleyParserWrapper(input):
  """
  Takes a set of strings as input and prints the resultant parse tree(s)
  generated by the Earley Parser in NLTK.

  Arguments:
    input: array of strings

  Output:
    Prints the the resultant parse tree(s) generated by the Earley Parser in NLTK.
  """

  # CFG
  grammarString = """
  S -> NP | VP | PP | NP VP | VP NP | NP ',' NP VP | NP ',' NP VP ',' VP | PP ',' S | S CC S| S ',' CC S | S PP | NP ',' PP ',' NP VP NP PP | S ',' V S

  NP -> ProN | PropN | N | DT NP | CC NP | ADJ NP | RB NP | N NP | PropN NP | ProN NP | NP PP
  ProN -> PRP | PRPS | WP | WPS
  PropN -> NNP | NNPS
  N -> NN | NNS

  VP -> V | V NP | V VP | V NP PP | V PP | V ADJ | V ADJ NP | RB VP | VP PP
  V -> VB | VBD | VBG | VBN | VBP | VBZ
  PP -> IN NP | IN VP | PP VP

  ADJ -> JJ | JJR | JJS
  """

  ### The variable tagset is defined in the file tagset to keep the code clean.
  ### tagset contains a list of POS tags from the Penn treebank and the corresponding list of words for each POS tag

  for POS in tagset.keys():
    for item in tagset[POS]:
      # Adding each word defined in the tagset as a terminal rule into the grammar
      grammarString += f"{POS} -> '{item}'\n"

  # Creating the CFG
  grammar = CFG.fromstring(grammarString)

  # Invoking the NLTK Earley Parser
  parser = nltk.parse.EarleyChartParser(grammar)

  # For each sentence group out of the nine given sentence groups
  for item in input:
    print("1")
    # For each sentence in a sentence group
    for sentence in item.split("."):
      # Output current sentence
      print(sentence.strip())

      # Replace ' with - to avoid errors while tokenizing
      sentence = sentence.replace("'", "-")

      tokens = Tokenizer(sentence)

      # Converting all non proper nouns to lower case
      for i in range(len(tokens)):
        if tokens[i] not in tagset["NNP"]:
          tokens[i] = tokens[i].lower()

      # Printing tree for set of tokens
      for x in parser.parse(tokens):
        print(TreePrettyPrinter(Tree.fromstring(str(x))).text())


# Driver Code
if __name__ == "__main__":
  input = [
    "John ate the apple at the table.",
    "John, who ate the apple from the fridge, was sick Wednesday.",
    "On Monday, John ate the apple in his office.",
    "Last week, on Monday, John finally took the apple from the fridge to his office.",
    "Heart rate greater than 90 beats/minute",
    "Body temperature over 38 or under 36 degrees Celsius.",
    "Respiratory rate greater than 20 breaths/minute or partial pressure of CO2 less than 32 mmHg",
    #"Leukocyte count greater than 12000 or less than 4000 /microliters or over 10% immature forms or bands."


    # Challenge Text

  ]


  earleyParserWrapper(input)

"""#To show the CFG output into CoNLL format"""

import spacy

# Load the spaCy model (e.g., 'en_core_web_sm')
nlp = spacy.load('en_core_web_sm')

# Define the context-free grammar for English
grammar = """
  S -> NP | VP | PP | NP VP | VP NP | NP ',' NP VP | NP ',' NP VP ',' VP | PP ',' S | S CC S| S ',' CC S | S PP | NP ',' PP ',' NP VP NP PP | S ',' V S

  NP -> ProN | PropN | N | DT NP | CC NP | ADJ NP | RB NP | N NP | PropN NP | ProN NP | NP PP
  ProN -> PRP | PRPS | WP | WPS
  PropN -> NNP | NNPS
  N -> NN | NNS

  VP -> V | V NP | V VP | V NP PP | V PP | V ADJ | V ADJ NP | RB VP | VP PP
  V -> VB | VBD | VBG | VBN | VBP | VBZ
  PP -> IN NP | IN VP | PP VP

  ADJ -> JJ | JJR | JJS
  """

# sentences
sentences = [
    "John ate the apple at the table.",
    "John, who ate the apple from the fridge, was sick Wednesday.",
    "On Monday, John ate the apple in his office.",
    "Last week, on Monday, John finally took the apple from the fridge to his office.",
    "Heart rate greater than 90 beats/minute",
    "Body temperature over 38 or under 36 degrees Celsius.",
    "Respiratory rate greater than 20 breaths/minute or partial pressure of CO2 less than 32 mmHg",
    "Leukocyte count greater than 12000 or less than 4000 /microliters or over 10% immature forms or bands."

                 ]

# Parse and annotate sentences
for sentence in sentences:
    # Process the sentence with spaCy
    doc = nlp(sentence)

    # Extract POS and NE information and annotate with Ent and MeasEnt
    token_index = 1
    for token in doc:
        word = token.text
        pos = token.pos_
        ne = token.ent_type_  # Named Entity (NE) label
        ent = "Ent"  # Entity Type (customize based on your data)
        meas_ent = "MeasEnt"  # Measured Entity (customize based on your data)

        # Format and print each token's information in CoNLL format
        print(f"{token_index}\t{word}\t_\t{pos}\t{ne}\t{ent}\t{meas_ent}")
        token_index += 1

    # Print an empty line to separate sentences
    print()